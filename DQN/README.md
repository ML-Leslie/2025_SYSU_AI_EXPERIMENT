## ğŸ“Œç›®å½•
- [ğŸ“Œç›®å½•](#ç›®å½•)
- [äººå·¥æ™ºèƒ½å®éªŒæŠ¥å‘Š å®éªŒäº” DNQ](#äººå·¥æ™ºèƒ½å®éªŒæŠ¥å‘Š-å®éªŒäº”-dnq)
  - [ä¸€ã€å®éªŒç›®çš„](#ä¸€å®éªŒç›®çš„)
  - [äºŒã€å®éªŒå†…å®¹](#äºŒå®éªŒå†…å®¹)
    - [1. ç®—æ³•åŸç†](#1-ç®—æ³•åŸç†)
    - [2. å…³é”®ä»£ç å±•ç¤º](#2-å…³é”®ä»£ç å±•ç¤º)
      - [(1) ä»£ç ç»“æ„](#1-ä»£ç ç»“æ„)
        - [ä»£ç æ¡†æ¶](#ä»£ç æ¡†æ¶)
        - [agent\_dqn.py](#agent_dqnpy)
      - [(2) å…³é”®ä»£ç ](#2-å…³é”®ä»£ç )
        - [argument.py](#argumentpy)
        - [QNetwork](#qnetwork)
        - [ReplayBuffer](#replaybuffer)
        - [AgentDQN](#agentdqn)
    - [3. åˆ›æ–°ç‚¹\&ä¼˜åŒ–](#3-åˆ›æ–°ç‚¹ä¼˜åŒ–)
  - [ä¸‰ã€å®éªŒç»“æœåŠåˆ†æ](#ä¸‰å®éªŒç»“æœåŠåˆ†æ)
    - [å®éªŒç»“æœ](#å®éªŒç»“æœ)
    - [è°ƒå‚](#è°ƒå‚)
  - [å››ã€å‚è€ƒæ–‡çŒ®](#å››å‚è€ƒæ–‡çŒ®)

---

## äººå·¥æ™ºèƒ½å®éªŒæŠ¥å‘Š å®éªŒäº” DNQ
### ä¸€ã€å®éªŒç›®çš„
- åœ¨ `CartPole-v0` ç¯å¢ƒä¸­å®ç° DQN ç®—æ³•
### äºŒã€å®éªŒå†…å®¹
#### 1. ç®—æ³•åŸç†
- DQNï¼ˆDeep Q-Networkï¼‰æ˜¯ä¸€ç§ç»“åˆäº†æ·±åº¦å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ç®—æ³•ã€‚å®ƒä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ Q å‡½æ•°ï¼Œä»è€Œè§£å†³é«˜ç»´çŠ¶æ€ç©ºé—´ä¸‹çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚
- DQN çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥ä¼°è®¡ Q å€¼ï¼Œå¹¶é€šè¿‡**ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œ**æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
- ä¸»è¦æ­¥éª¤åŒ…æ‹¬ï¼š
  - **ç»éªŒå›æ”¾**ï¼šå°†æ™ºèƒ½ä½“çš„ç»å†å­˜å‚¨åœ¨ä¸€ä¸ªç¼“å†²åŒºä¸­ï¼ŒéšæœºæŠ½å–å°æ‰¹é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä»¥æ‰“ç ´æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
  - **ç›®æ ‡ç½‘ç»œ**ï¼šä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„ç›®æ ‡ç½‘ç»œæ¥è®¡ç®— Q å€¼ï¼Œå®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œçš„æƒé‡ï¼Œä»¥å‡å°‘è®­ç»ƒçš„ä¸ç¨³å®šæ€§ã€‚
  - **Q-learning æ›´æ–°**ï¼šä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹æ›´æ–° Q å€¼ã€‚ï¼ˆ $Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)$ ï¼‰
- DQN çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š
  - åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒºå’Œç›®æ ‡ç½‘ç»œã€‚
  - åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œé€‰æ‹©åŠ¨ä½œå¹¶æ‰§è¡Œï¼Œå­˜å‚¨çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€åˆ°ç»éªŒå›æ”¾ç¼“å†²åŒºã€‚
  - ä»ç»éªŒå›æ”¾ç¼“å†²åŒºéšæœºæŠ½å–å°æ‰¹é‡æ ·æœ¬ã€‚
  - ä½¿ç”¨å½“å‰ç½‘ç»œè®¡ç®— Q å€¼ï¼Œå¹¶ä½¿ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—ç›®æ ‡ Q å€¼ã€‚
  - æ›´æ–°å½“å‰ç½‘ç»œçš„æƒé‡ï¼Œä½¿å…¶æ›´æ¥è¿‘ç›®æ ‡ Q å€¼ã€‚
  - å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œçš„æƒé‡ã€‚
  - é‡å¤ä»¥ä¸Šæ­¥éª¤ç›´åˆ°è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶ã€‚
#### 2. å…³é”®ä»£ç å±•ç¤º
##### (1) ä»£ç ç»“æ„
###### ä»£ç æ¡†æ¶
- ç»™å‡ºçš„ä»£ç æ¡†æ¶ç»“æ„æ˜¯è¿™æ ·çš„ï¼š
    ```bash
    DQN/
    â”œâ”€â”€ agent_dir/          # æ™ºèƒ½ä½“ç›®å½•
    â”‚   â”œâ”€â”€ agent.py        
    â”‚   â””â”€â”€ agent_dqn.py    # DQN æ™ºèƒ½ä½“å®ç°
    â”œâ”€â”€ argument.py         # å‚æ•°é…ç½®æ–‡ä»¶
    â””â”€â”€ main.py             # ä¸»ç¨‹åºå…¥å£
    ```
  - æ³¨ï¼šæœ¬æ¬¡å®éªŒåªå®ç°äº† DQN æ™ºèƒ½ä½“
###### agent_dqn.py
- è¯¥æ–‡ä»¶å®ç°äº† DQN æ™ºèƒ½ä½“çš„æ ¸å¿ƒé€»è¾‘ï¼ŒåŒ…æ‹¬åˆå§‹åŒ–æ¸¸æˆè®¾ç½®ã€è®­ç»ƒã€åŠ¨ä½œé€‰æ‹©ã€è¿è¡Œå’Œç»˜å›¾ç­‰åŠŸèƒ½ã€‚
- ä»¥ä¸‹æ˜¯è¯¥æ–‡ä»¶çš„ä¸»è¦å†…å®¹ï¼š
    ```bash
    AgentDQN
        â”œâ”€â”€ __init__
        â”œâ”€â”€ init_game_setting
        â”œâ”€â”€ train
        â”œâ”€â”€ make_action
        â”œâ”€â”€ run
        â””â”€â”€ plot

    QNetwork
        â”œâ”€â”€ __init__
        â””â”€â”€ forward

    ReplayBuffer
        â”œâ”€â”€ __init__
        â”œâ”€â”€ __len__
        â”œâ”€â”€ push
        â”œâ”€â”€ sample
        â”œâ”€â”€ update_priorities
        â””â”€â”€ clean
    ``` 
- ä¸»è¦åŒ…å«ä¸‰ä¸ªç±»ï¼š`AgentDQN`ã€`QNetwork` å’Œ `ReplayBuffer`ï¼Œåˆ†åˆ«ç”¨äºå®ç° DQN æ™ºèƒ½ä½“ã€Q ç½‘ç»œå’Œç»éªŒå›æ”¾ç¼“å†²åŒºã€‚
##### (2) å…³é”®ä»£ç 
###### argument.py
- è¯¥æ–‡ä»¶å®šä¹‰äº† DQN æ™ºèƒ½ä½“çš„å‚æ•°é…ç½®ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ã€epsilon-greedy ç­–ç•¥çš„å‚æ•°ç­‰ã€‚
- ä»£ç ï¼š
    ```python
    def dqn_arguments(parser):
        """
        Add your arguments here if needed. The TAs will run test.py to load
        your default arguments.

        For example:
            parser.add_argument('--batch_size', type=int, default=32, help='batch size for training')
            parser.add_argument('--learning_rate', type=float, default=0.01, help='learning rate for training')
        """
        parser.add_argument('--env_name', default="CartPole-v0", help='environment name')

        parser.add_argument("--seed", default=11037, type=int)
        parser.add_argument("--hidden_size", default=128, type=int)  
        parser.add_argument("--lr", default=1e-3, type=float) 
        parser.add_argument("--gamma", default=0.99, type=float)
        parser.add_argument("--grad_norm_clip", default=1.0, type=float)

        parser.add_argument("--test", default=False, type=bool)
        parser.add_argument("--use_cuda", default=True, type=bool)
        parser.add_argument("--n_frames", default=int(100000), type=int) 

        # DQN specific arguments
        parser.add_argument("--input_size", type=int, default=4, help='input_size for training')
        parser.add_argument("--convergence_threshold", default=190, type=int, help='convergence threshold for rewards')
        parser.add_argument("--consecutive_episodes", default=20, type=int, help='consecutive episodes for convergence')
        parser.add_argument('--target_update_freq', type=int, default=500, help='frequency to update target network')
        parser.add_argument('--batch_size', type=int, default=128, help='batch size for training')
        parser.add_argument('--buffer_size', type=int, default=100000, help='replay buffer size')
        parser.add_argument('--epsilon_start', type=float, default=0.9, help='start value of epsilon')
        parser.add_argument('--epsilon_end', type=float, default=0.01, help='end value of epsilon') 
        parser.add_argument('--epsilon_decay', type=int, default=10000, help='epsilon decay rate') 

        return parser
    ```
- å„ç§å‚æ•°ä½œç”¨åŠæ•°å€¼å¦‚ä¸‹è¡¨æ ¼ï¼š

    |       å‚æ•°åç§°       |               ä½œç”¨                |     æ•°å€¼      |
    | :------------------: | :-------------------------------: | :-----------: |
    |      `env_name`      |             ç¯å¢ƒåç§°              | `CartPole-v0` |
    |        `seed`        |             éšæœºç§å­              |    `11037`    |
    |    `hidden_size`     |         Q ç½‘ç»œéšè—å±‚å¤§å°          |     `128`     |
    |         `lr`         |              å­¦ä¹ ç‡               |    `1e-3`     |
    |       `gamma`        |             æŠ˜æ‰£å› å­              |    `0.99`     |
    |   `grad_norm_clip`   |           æ¢¯åº¦è£å‰ªé˜ˆå€¼            |     `1.0`     |
    |        `test`        |           æ˜¯å¦æµ‹è¯•æ¨¡å¼            |    `False`    |
    |      `use_cuda`      |           æ˜¯å¦ä½¿ç”¨ CUDA           |    `True`     |
    |      `n_frames`      |             è®­ç»ƒå¸§æ•°              |   `100000`    |
    |     `input_size`     |    è¾“å…¥å±‚å¤§å°ï¼ˆçŠ¶æ€ç©ºé—´ç»´åº¦ï¼‰     |      `4`      |
    | `target_update_freq` |     ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰      |     `500`     |
    |     `batch_size`     |             æ‰¹é‡å¤§å°              |     `128`     |
    |    `buffer_size`     |        ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°         |   `100000`    |
    |   `epsilon_start`    |     epsilon-greedy ç­–ç•¥èµ·å§‹å€¼     |     `0.9`     |
    |    `epsilon_end`     |     epsilon-greedy ç­–ç•¥ç»“æŸå€¼     |    `0.01`     |
    |   `epsilon_decay`    | epsilon-greedy ç­–ç•¥è¡°å‡ç‡ï¼ˆæ­¥æ•°ï¼‰ |    `10000`     |

  - è¿™é‡Œè®²è®² `epsilon` ç­–ç•¥çš„å‚æ•°è®¾ç½®ï¼š`epsilon` æ˜¯ DQN ä¸­ç”¨äºå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨çš„å‚æ•°
  - å¦‚æœæœ«å€¼è®¾ç½®å¾—å¤ªé«˜ï¼Œå®¹æ˜“åœ¨åé¢æ”¶æ•›æ—¶å‡ºç°ä¸ç¨³å®šçš„ç°è±¡ï¼›è€Œ `epsilon` æœ«å€¼è®¾ç½®å¾—å¤ªä½ï¼Œåˆ™å¯èƒ½å¯¼è‡´æ™ºèƒ½ä½“è¿‡æ—©åœ°é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚
  - `epsilon_decay` æ˜¯è¡°å‡çš„é€Ÿç‡ï¼Œå¦‚æœè¡°å‡åœ°è¿‡å¿«ï¼Œå‰æœŸå¯èƒ½ä¼šæ²¡æœ‰å­¦ä¹ åˆ°è¾ƒå¥½çš„ç­–ç•¥ï¼ˆè™½ç„¶åœ¨è¿™ä¸ªå®éªŒ`CartPole-v0` ä¸­ï¼Œæ™ºèƒ½ä½“çš„å­¦ä¹ é€Ÿåº¦è¾ƒå¿«ï¼‰
  - ç»è¿‡å¤šæ¬¡å®éªŒï¼Œæœ€ç»ˆé€‰æ‹©äº†ä¸Šè¿°å‚æ•°è®¾ç½®ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨ `CartPole-v0` ç¯å¢ƒä¸­èƒ½å¤Ÿè¾ƒå¥½åœ°å­¦ä¹ åˆ°å¹³è¡¡æ†çš„ç­–ç•¥ã€‚ï¼ˆè°ƒå‚éƒ¨åˆ†è§ â†“ [ä¸‰ã€å®éªŒç»“æœåŠåˆ†æ](#è°ƒå‚)ï¼‰
###### QNetwork 
- ä»£ç ï¼š
    ```python
    class QNetwork(nn.Module):
        def __init__(self, input_dim, hidden_dim, output_dim):
            super().__init__()
            self.fc1 = nn.Linear(input_dim, hidden_dim * 2)
            self.relu = nn.ReLU()
            self.fc2 = nn.Linear(hidden_dim * 2, output_dim)
            # æƒé‡æ­£äº¤åˆå§‹åŒ–
            nn.init.orthogonal_(self.fc1.weight, gain=1.0)
            nn.init.orthogonal_(self.fc2.weight, gain=1.0)

        def forward(self, x):
            x = self.fc1(x)
            x = self.relu(x)
            x = self.fc2(x)
            return x
    ```
    - è¯¥ç±»å®šä¹‰äº† Q ç½‘ç»œçš„ç»“æ„ï¼ŒåŒ…æ‹¬è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ã€‚ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°
    - ç»è¿‡ä¸æ–­æµ‹è¯•ï¼Œå‘ç°å¯¹äºè§£å†³ `CartPole-v0` ç¯å¢ƒï¼Œä½¿ç”¨ä¸¤ä¸ªéšè—å±‚çš„ç»“æ„æ•ˆæœè¾ƒå¥½ï¼Œä¸å¿…è¿‡äºå¤æ‚
    - åŒæ—¶ä½¿ç”¨**æ­£äº¤åˆå§‹åŒ–**æ¥åˆå§‹åŒ–æƒé‡ï¼Œä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚[^1]
###### ReplayBuffer
- ä»£ç ï¼š
    ```py
    class ReplayBuffer:
        def __init__(self, buffer_size, alpha=0.6):
            self.buffer = deque(maxlen=buffer_size)
            self.priorities = deque(maxlen=buffer_size)
            self.alpha = alpha  # å†³å®šä¼˜å…ˆçº§çš„ç¨‹åº¦
            
        def __len__(self):
            return len(self.buffer)
        
        def push(self, *transition):
            max_priority = max(self.priorities) if self.priorities else 1.0
            self.buffer.append(transition)
            self.priorities.append(max_priority)
        
        def sample(self, batch_size, beta=0.4):
            # è®¡ç®—é‡‡æ ·æ¦‚ç‡
            if len(self.buffer) == 0:
                return [], [], []
            
            priorities = np.array(self.priorities, dtype=np.float32)
            probs = priorities ** self.alpha
            probs /= probs.sum()
            
            # é‡‡æ ·ç´¢å¼•å’Œè®¡ç®—é‡è¦æ€§æƒé‡
            indices = np.random.choice(len(self.buffer), batch_size, p=probs)
            samples = [self.buffer[idx] for idx in indices]
            
            # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
            weights = (len(self.buffer) * probs[indices]) ** (-beta)
            weights /= weights.max()  # å½’ä¸€åŒ–æƒé‡
            
            return samples, indices, weights
        def update_priorities(self, indices, priorities):
            # ç¡®ä¿prioritiesæ˜¯æ ‡é‡å€¼
            for idx, priority in zip(indices, priorities.flatten() if hasattr(priorities, 'flatten') else priorities):
                if idx < len(self.priorities):
                    # å°†ä¼˜å…ˆçº§ä½œä¸ºæ ‡é‡å€¼å­˜å‚¨
                    self.priorities[idx] = float(priority)
            
        def clean(self):
            self.buffer.clear()
            self.priorities.clear()
    ```
- è¯¥ç±»å®ç°äº†**ç»éªŒå›æ”¾ç¼“å†²åŒº** [^2]
- **ç»éªŒå›æ”¾ç¼“å†²åŒº**ç›¸è¾ƒäºæ™®é€šçš„ç¼“å†²åŒºç­–ç•¥çš„ä¼˜åŠ¿åœ¨äºï¼šé«˜ TD-error çš„ transition æ›´å®¹æ˜“è¢«é‡‡æ ·ï¼Œæ¨¡å‹èƒ½æ›´å¿«ä¿®æ­£â€œå¤§é”™â€ï¼Œä¸”èƒ½é‡å¤åˆ©ç”¨é‡è¦çš„ç»éªŒ
- åœ¨ `push` è¿›å…¥ç¼“å†²åŒºçš„æ—¶å€™å°±è§„å®šäº†ä¼˜å…ˆçº§ä¸ºå½“å‰ç¼“å†²åŒºä¸­æœ€å¤§çš„ä¼˜å…ˆçº§ï¼Œä¿è¯äº†æ–°è¿›å…¥çš„ transition åœ¨é‡‡æ ·æ—¶æœ‰è¾ƒé«˜çš„æ¦‚ç‡è¢«é€‰ä¸­
- `sample` æ–¹æ³•ä¸­,é¦–å…ˆç”¨ `priorities ** self.alpha` è®¡ç®—æ¯ä¸ª transition çš„é‡‡æ ·æ¦‚ç‡ï¼Œç„¶åæ ¹æ®è¿™äº›æ¦‚ç‡è¿›è¡Œé‡‡æ ·ï¼ˆä½¿ç”¨ `alpha` å‚æ•°æ¥æ§åˆ¶ä¼˜å…ˆçº§çš„å½±å“ç¨‹åº¦ï¼Œæ³¨æ„æœ€åå½’ä¸€åŒ–ï¼‰ï¼›æ¥ç€ä½¿ç”¨ `np.random.choice` æ ¹æ®é‡‡æ ·æ¦‚ç‡éšæœºé€‰æ‹© `batch_size` ä¸ª transitionï¼Œç”±äºé‡‡æ ·æ¦‚ç‡ä¸å‡åŒ€ï¼Œä¼šå¼•å…¥åå·®ï¼Œæ‰€ä»¥å¼•å…¥ **é‡è¦æ€§é‡‡æ ·æƒé‡** ï¼Œæƒé‡è®¡ç®—å…¬å¼ä¸ºï¼š`(N * P(i))^(-beta)`ï¼Œå…¶ä¸­ N æ˜¯ç¼“å†²åŒºå¤§å°ï¼ŒP(i) æ˜¯ç¬¬ i ä¸ª transition çš„é‡‡æ ·æ¦‚ç‡ï¼Œ`beta` æ§åˆ¶ä¿®æ­£ç¨‹åº¦ï¼Œè¿™äº›æƒé‡ä¼šåœ¨è®­ç»ƒæŸå¤±ä¸­ä½œä¸ºåŠ æƒå› å­ï¼Œä¿®æ­£é‡‡æ ·åå·®
- `update_priorities` æ ¹æ® TD-error ç­‰æŒ‡æ ‡ï¼Œæ›´æ–°é‡‡æ ·è¿‡çš„ transition çš„ä¼˜å…ˆçº§ï¼ˆTD-error æ˜¯æŒ‡å½“å‰ Q å€¼å’Œç›®æ ‡ Q å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—ï¼‰
###### AgentDQN
- ä» `run` å‡½æ•°ä¸€æ­¥æ­¥æ‹†è§£
    ```python
    def run(self):
        state = self.env.reset() 
        episode_reward = 0
        episode_count = 0

        # ç”»å›¾è®°å½•å‚æ•°
        rewards = []

        while self.total_steps < self.args.n_frames:
            if not self.args.test:
                self.epsilon = self.args.epsilon_end + (self.args.epsilon_start - self.args.epsilon_end) * \
                    math.exp(-1. * self.total_steps / self.args.epsilon_decay)
            # TODO
            action = self.make_action(state, test=self.args.test)
            next_state, reward, done, _ = self.env.step(action)

            if not self.args.test: # å¦‚æœä¸æ˜¯æµ‹è¯•æ¨¡å¼
                self.replay_buffer.push(state, action, reward, next_state, done) # å°†è½¬æ¢å­˜å‚¨åˆ°ç»éªŒå›æ”¾ç¼“å†²åŒº
                self.train() # è®­ç»ƒæ¨¡å‹

            state = next_state # æ›´æ–°å½“å‰çŠ¶æ€
            episode_reward += reward #ç´¯ç§¯å›åˆå¥–åŠ±
            self.total_steps += 1 # å¢åŠ æ€»æ­¥æ•°

            if done: # å¦‚æœå›åˆç»“æŸ
                episode_count += 1 
                rewards.append(episode_reward)

                print(f"Episode {episode_count},total_steps:{self.total_steps}, \
                    Total reward: {episode_reward},epsilon: {self.epsilon:.4f}")

                # é‡ç½®çŠ¶æ€å’Œå¥–åŠ±
                state = self.env.reset()
                episode_reward = 0

        self.plot(rewards)
        return self
    ```
- é¦–å…ˆæˆ‘ä»¬åˆå§‹åŒ–ç¯å¢ƒï¼Œè·å–åˆå§‹çŠ¶æ€ï¼Œå¹¶è®¾ç½®ä¸€äº›å˜é‡æ¥è®°å½•å›åˆå¥–åŠ±å’Œå›åˆæ•°
- æ¥ç€è¿›å…¥ä¸€ä¸ªå¾ªç¯ï¼Œç›´åˆ°æ€»æ­¥æ•°è¾¾åˆ°é¢„è®¾çš„å¸§æ•°ï¼ˆä¹Ÿå°±æ˜¯è®­ç»ƒè¿‡ç¨‹ï¼‰
- å¦‚æœä¸æ˜¯æµ‹è¯•æ¨¡å¼ï¼ˆè¿™é‡Œæˆ‘ä»¬ä¸€ç›´è®¾ç½®ä¸º falseï¼‰ ï¼Œåˆ™æ ¹æ®å½“å‰æ€»æ­¥æ•°è®¡ç®— epsilon-greedy ç­–ç•¥çš„ epsilon å€¼ï¼Œé‡‡ç”¨æŒ‡æ•°è¡°å‡çš„æ–¹å¼[^3] ï¼Œè¿™ç§æ–¹å¼çš„ä¼˜ç‚¹æ˜¯å‰æœŸè¡°å‡è¾ƒæ…¢ï¼Œèƒ½å­¦ä¹ åˆ°æ›´å¤šçš„ç­–ç•¥ï¼Œè€ŒåæœŸè¡°å‡è¾ƒå¿«ï¼Œèƒ½æ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥
- æ¥ç€è°ƒç”¨ `make_action` å‡½æ•°é€‰æ‹©åŠ¨ä½œï¼Œå¹¶æ‰§è¡Œè¯¥åŠ¨ä½œï¼Œè·å–ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±å’Œæ˜¯å¦ç»“æŸçš„æ ‡å¿—
- `make_action` å‡½æ•°æ ¹æ® epsilon-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œï¼Œå¦‚æœéšæœºæ•°å°äº epsilonï¼Œåˆ™éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œå¦åˆ™é€‰æ‹©å½“å‰ Q ç½‘ç»œé¢„æµ‹çš„æœ€ä¼˜åŠ¨ä½œ
    ```py
    def make_action(self, observation, test=True): #TODO
        """
        è¿”å›æ™ºèƒ½ä½“çš„é¢„æµ‹åŠ¨ä½œ
        è¾“å…¥: observation (è§‚å¯Ÿå€¼)
        è¿”å›: action (åŠ¨ä½œ)
        """
        if not test: # è®­ç»ƒæ—¶çš„ Epsilon-greedy ç­–ç•¥
            if random.random() < self.epsilon: 
                return self.env.action_space.sample()
        
        # æµ‹è¯•æ—¶æˆ–ä¸è¿›è¡Œæ¢ç´¢æ—¶çš„è´ªå©ªåŠ¨ä½œ
        with torch.no_grad(): # åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ä¸è®¡ç®—æ¢¯åº¦
            observation = torch.FloatTensor(observation).unsqueeze(0).to(self.device) 
            q_values = self.q_network(observation) 
            return q_values.max(1)[1].item() 
    ```
- å¦‚æœä¸æ˜¯æµ‹è¯•æ¨¡å¼ï¼Œåˆ™å°†å½“å‰çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œæ˜¯å¦ç»“æŸçš„æ ‡å¿—å­˜å‚¨åˆ°ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œå¹¶è°ƒç”¨ `train` å‡½æ•°è¿›è¡Œè®­ç»ƒ
- `train` å‡½æ•°çš„ä¸»è¦é€»è¾‘æ˜¯ä»ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·ä¸€æ‰¹æ•°æ®ï¼Œè®¡ç®— Q å€¼å’Œç›®æ ‡ Q å€¼ï¼Œå¹¶æ›´æ–° Q ç½‘ç»œçš„æƒé‡ï¼Œå…¶ä¸­æˆ‘ä»¬è®¡ç®—æŸå¤±é‡‡ç”¨çš„æ˜¯åŠ æƒçš„ Huber æŸå¤±ï¼Œå¯ä»¥æ›´å¥½åœ°å¤„ç†å¼‚å¸¸å€¼ï¼›ä½¿ç”¨ `Adam` ä¼˜åŒ–å™¨è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨æ¢¯åº¦è£å‰ªæ¥é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    ```py
    def train(self):
        if len(self.replay_buffer) < self.args.batch_size:
            return
        
        # ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾
        transitions, indices, weights = self.replay_buffer.sample(self.args.batch_size)
        batch = list(zip(*transitions))
        
        state_batch = torch.FloatTensor(np.array(batch[0])).to(self.device)
        action_batch = torch.LongTensor(np.array(batch[1])).unsqueeze(1).to(self.device) #TODO
        reward_batch = torch.FloatTensor(np.array(batch[2])).unsqueeze(1).to(self.device)
        next_state_batch = torch.FloatTensor(np.array(batch[3])).to(self.device)
        done_batch = torch.FloatTensor(np.array(batch[4])).unsqueeze(1).to(self.device)

        q_values = self.q_network(state_batch).gather(1, action_batch)

        with torch.no_grad():
            next_state_actions = self.q_network(next_state_batch).max(1)[1].unsqueeze(1)
            next_q_values = self.target_q_network(next_state_batch).gather(1, next_state_actions)

        expected_q_values = reward_batch + (self.args.gamma * next_q_values * (1 - done_batch))
        
        # è®¡ç®—TDè¯¯å·®
        td_errors = torch.abs(q_values - expected_q_values).detach().cpu().numpy()
        # æ›´æ–°ä¼˜å…ˆçº§
        new_priorities = td_errors + 1e-6  # æ·»åŠ å°å¸¸æ•°é˜²æ­¢ä¼˜å…ˆçº§ä¸º0
        self.replay_buffer.update_priorities(indices, new_priorities)

        # åŠ æƒHuberæŸå¤±
        weights = torch.FloatTensor(weights).unsqueeze(1).to(self.device)
        loss = (weights * nn.functional.smooth_l1_loss(q_values, expected_q_values, reduction='none')).mean()

        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), self.args.grad_norm_clip) # æ¢¯åº¦è£å‰ª
        self.optimizer.step()

        self.total_steps += 1
        if self.total_steps % self.args.target_update_freq == 0:
            self.target_q_network.load_state_dict(self.q_network.state_dict())
            
        return loss.item()
    ```
- æ¯ä¸ªå›åˆç»“æŸåï¼Œæ‰“å°å½“å‰å›åˆæ•°ã€æ€»æ­¥æ•°ã€æ€»å¥–åŠ±å’Œ epsilon å€¼ï¼Œå¹¶é‡ç½®çŠ¶æ€å’Œå¥–åŠ±
- æœ€åè°ƒç”¨ `plot` å‡½æ•°ç»˜åˆ¶å¥–åŠ±æ›²çº¿
#### 3. åˆ›æ–°ç‚¹&ä¼˜åŒ–
- ä¼˜åŒ–1ï¼š**æ­£äº¤åˆå§‹åŒ–**ï¼ˆorthogonal initializationï¼‰[^1]
  - åœ¨ Q ç½‘ç»œçš„æƒé‡åˆå§‹åŒ–æ—¶ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–ï¼Œå¯ä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦
- ä¼˜åŒ–2ï¼š**ç»éªŒå›æ”¾ç¼“å†²åŒº**ï¼ˆReplay Bufferï¼‰[^2]
  - ä½¿ç”¨ç»éªŒå›æ”¾ç¼“å†²åŒºæ¥å­˜å‚¨æ™ºèƒ½ä½“çš„ç»å†ï¼Œå¹¶ä»ä¸­éšæœºé‡‡æ ·è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æ‰“ç ´æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæé«˜è®­ç»ƒæ•ˆæœ
- ä¼˜åŒ–3ï¼š**æŒ‡æ•°è¡°å‡ç­–ç•¥**ï¼ˆExponential Decay Strategyï¼‰[^3]
  - åœ¨ epsilon-greedy ç­–ç•¥ä¸­ä½¿ç”¨æŒ‡æ•°è¡°å‡çš„æ–¹å¼æ¥è°ƒæ•´ epsilon å€¼ï¼Œå¯ä»¥åœ¨è®­ç»ƒåˆæœŸè¿›è¡Œæ›´å¤šçš„æ¢ç´¢ï¼Œè€Œåœ¨åæœŸæ›´å€¾å‘äºåˆ©ç”¨å·²å­¦åˆ°çš„ç­–ç•¥
### ä¸‰ã€å®éªŒç»“æœåŠåˆ†æ
#### å®éªŒç»“æœ
- ç”±äºå®éªŒéšæœºæ€§å¾ˆé«˜ï¼Œæ€»å…±è¿›è¡Œäº† 9 æ¬¡è¯•éªŒï¼Œæ¯æ¬¡å¾—åˆ° rewardæ›²çº¿å›¾ å¦‚ä¸‹ï¼š
  - ![alt text](images/image.png)
  - ![alt text](images/image-1.png)
  - ![alt text](images/image-2.png)
  - ![alt text](images/image-3.png)
  - ![alt text](images/image-6.png)
  - ![alt text](images/image-11.png)
  - ![alt text](images/image-12.png)
  - ![alt text](images/image-15.png)
  - ![alt text](images/image-16.png)
- ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œè¾¾åˆ°äº†è®­ç»ƒç›®çš„ï¼Œèƒ½å¤Ÿæ”¶æ•›åˆ° 200 reward å·¦å³
#### è°ƒå‚
> - è®°å½•æˆ‘å‚æ•°è°ƒæ•´è¿‡ç¨‹
- ä¸€å¼€å§‹æˆ‘çš„å‚æ•°è®¾ç½®ä¸ºï¼šï¼ˆå…¶ä»–å‚æ•°ä¸€è‡´ï¼‰
  - `n_frames=30000`ï¼Œ`epsilon_start=0.9`ï¼Œ`epsilon_end=0.05`ï¼Œ`epsilon_decay=1000`
  - å¾—åˆ°çš„ reward æ›²çº¿å›¾å¦‚ä¸‹ï¼š
    - ![alt text](images/image-17.png)
    - å‘ç°çŸ­æš‚æ”¶æ•›åˆ° 200 å·¦å³ååˆå¼€å§‹ä¸‹é™ï¼ŒçŒœæƒ³åº”è¯¥æ˜¯è®­ç»ƒäº¤äº’æ­¥æ•°å¤ªå°‘äº†ï¼Œäºæ˜¯æˆ‘æƒ³æŠŠè®­ç»ƒå¸§æ•°æé«˜ï¼Œä»¥é˜²å‡ºç°â€œå‡æ”¶æ•›â€
- äºæ˜¯æˆ‘å°† `n_frames` è°ƒæ•´ä¸º `100000`ï¼Œå…¶ä»–å‚æ•°ä¸å˜ï¼Œå¾—åˆ°çš„ reward æ›²çº¿å›¾å¦‚ä¸‹ï¼š
  - ![alt text](images/image-7.png)
  - å‘ç°ç¨³å®šåº¦å¾ˆä½ï¼Œå¤šæ¬¡åˆ°è¾¾ 200 å·¦å³ååˆä¸‹é™ï¼ŒçŒœæƒ³å¯èƒ½æ˜¯ `epsilon` è¡°å‡å¤ªå¿«äº†ï¼Œäºæ˜¯æˆ‘å°† `epsilon_decay` è°ƒæ•´ä¸º `10000`ï¼Œç„¶åå°† `epsilon_end` è°ƒæ•´ä¸º `0.01`ï¼Œé˜²æ­¢åœ¨åæœŸæœ‰è¿‡åº¦æ¢ç´¢çš„æƒ…å†µ
- è°ƒæ•´åå‘ç°æœ‰æ‰€æ”¹å–„ï¼Œäºæ˜¯è¿›è¡Œå¤šæ¬¡è¯•éªŒï¼šâ†‘ [å®éªŒç»“æœ](#å®éªŒç»“æœ)
### å››ã€å‚è€ƒæ–‡çŒ®
> - https://www.gymlibrary.dev/
> - https://www.gymlibrary.dev/environments/classic_control/cart_pole/



[^1]:ä¼˜åŒ–1ï¼šæ­£äº¤åˆå§‹åŒ–
[^2]:ä¼˜åŒ–2ï¼šç»éªŒå›æ”¾ç¼“å†²åŒº
[^3]:ä¼˜åŒ–3ï¼šæŒ‡æ•°è¡°å‡ç­–ç•¥

