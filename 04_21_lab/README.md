## ðŸ“Œç›®å½•
- [ðŸ“Œç›®å½•](#ç›®å½•)
- [äººå·¥æ™ºèƒ½å®žéªŒæŠ¥å‘Š å®žéªŒå›› å¤šå±‚æ„ŸçŸ¥æœº](#äººå·¥æ™ºèƒ½å®žéªŒæŠ¥å‘Š-å®žéªŒå››-å¤šå±‚æ„ŸçŸ¥æœº)
  - [ä¸€.å®žéªŒé¢˜ç›®](#ä¸€å®žéªŒé¢˜ç›®)
  - [äºŒ.å®žéªŒå†…å®¹](#äºŒå®žéªŒå†…å®¹)
    - [1. ç®—æ³•åŽŸç†](#1-ç®—æ³•åŽŸç†)
      - [ï¼ˆ1ï¼‰å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰](#1å¤šå±‚æ„ŸçŸ¥æœºmlp)
      - [ï¼ˆ2ï¼‰æ¶‰åŠåˆ°çš„ä¸€äº›å‡½æ•°](#2æ¶‰åŠåˆ°çš„ä¸€äº›å‡½æ•°)
    - [2. å…³é”®ä»£ç å±•ç¤º](#2-å…³é”®ä»£ç å±•ç¤º)
      - [ä»£ç ç»“æž„](#ä»£ç ç»“æž„)
      - [å…³é”®å‡½æ•°](#å…³é”®å‡½æ•°)
    - [3. åˆ›æ–°ç‚¹\&ä¼˜åŒ–](#3-åˆ›æ–°ç‚¹ä¼˜åŒ–)
  - [ä¸‰.å®žéªŒç»“æžœåŠåˆ†æž](#ä¸‰å®žéªŒç»“æžœåŠåˆ†æž)
    - [1. å¯¹æ¯”ä¸åŒæ„ŸçŸ¥æœºå±‚æ•°ï¼Œå­¦ä¹ çŽ‡çš„ç»“æžœï¼ˆä½¿ç”¨`mini-batch`è®­ç»ƒï¼‰](#1-å¯¹æ¯”ä¸åŒæ„ŸçŸ¥æœºå±‚æ•°å­¦ä¹ çŽ‡çš„ç»“æžœä½¿ç”¨mini-batchè®­ç»ƒ)
      - [å®žéªŒç¼–å· 1](#å®žéªŒç¼–å·-1)
      - [å®žéªŒç¼–å· 2](#å®žéªŒç¼–å·-2)
      - [å®žéªŒç¼–å· 3](#å®žéªŒç¼–å·-3)
      - [å®žéªŒç¼–å· 4](#å®žéªŒç¼–å·-4)
      - [å¯¹æ¯”æ€»ç»“](#å¯¹æ¯”æ€»ç»“)
      - [é™„ï¼š](#é™„)
    - [2. å¯¹æ¯”`mini-batch`å’Œå…¨é‡è®­ç»ƒçš„ç»“æžœ](#2-å¯¹æ¯”mini-batchå’Œå…¨é‡è®­ç»ƒçš„ç»“æžœ)
      - [å®žéªŒç¼–å· 1](#å®žéªŒç¼–å·-1-1)
        - [ä¸ä½¿ç”¨`mini-batch`è®­ç»ƒ](#ä¸ä½¿ç”¨mini-batchè®­ç»ƒ)
      - [å®žéªŒç¼–å· 3](#å®žéªŒç¼–å·-3-1)
        - [ä¸ä½¿ç”¨`mini-batch`è®­ç»ƒ](#ä¸ä½¿ç”¨mini-batchè®­ç»ƒ-1)
      - [æ€»ç»“](#æ€»ç»“)
  - [å››.å‚è€ƒèµ„æ–™](#å››å‚è€ƒèµ„æ–™)

---

## äººå·¥æ™ºèƒ½å®žéªŒæŠ¥å‘Š å®žéªŒå›› å¤šå±‚æ„ŸçŸ¥æœº
### ä¸€.å®žéªŒé¢˜ç›®
- ç¼–å†™å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ç¨‹åºï¼Œå®žçŽ°æˆ¿ä»·é¢„æµ‹
- ç»˜åˆ¶æ•°æ®å¯è§†åŒ–å›¾ã€loss æ›²çº¿å›¾
### äºŒ.å®žéªŒå†…å®¹
#### 1. ç®—æ³•åŽŸç†
##### ï¼ˆ1ï¼‰å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰
- å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ˜¯ä¸€ä¸ªå‰é¦ˆç¥žç»ç½‘ç»œï¼Œç”±è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆã€‚
- æ¯ä¸€å±‚çš„ç¥žç»å…ƒä¸Žä¸‹ä¸€å±‚çš„ç¥žç»å…ƒä¹‹é—´éƒ½æœ‰è¿žæŽ¥ã€‚è®¾ç½®**æƒé‡å’Œåç½®**æ¥è°ƒæ•´æ¯ä¸ªç¥žç»å…ƒçš„è¾“å‡ºï¼Œç„¶åŽé€šè¿‡**æ¿€æ´»å‡½æ•°**è¿›è¡Œéžçº¿æ€§å˜æ¢ã€‚
- MLP çš„è®­ç»ƒè¿‡ç¨‹ä½¿ç”¨**å‰å‘ä¼ æ’­ + åå‘ä¼ æ’­**ç®—æ³•
  - **å‰å‘ä¼ æ’­**ï¼šè¾“å…¥æ•°æ®é€šè¿‡ç½‘ç»œå±‚å±‚ä¼ é€’ï¼Œè®¡ç®—å‡ºè¾“å‡ºç»“æžœã€‚
  - **åå‘ä¼ æ’­**ï¼šæ ¹æ®è¾“å‡ºç»“æžœå’ŒçœŸå®žæ ‡ç­¾è®¡ç®—æŸå¤±å‡½æ•°ï¼Œç„¶åŽé€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼Œæ›´æ–°å‚æ•°ã€‚ã€æ¢¯åº¦ä¸‹é™çš„å…¬å¼ä¸ºï¼š $W = W - \eta \frac{\partial L}{\partial W}$ å…¶ä¸­ $\eta$ ä¸ºå­¦ä¹ çŽ‡ã€‘
- é€šè¿‡ä¸æ–­è¿­ä»£æ›´æ–°å‚æ•°ï¼Œæœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œä»Žè€Œä½¿æ¨¡åž‹æ›´å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
##### ï¼ˆ2ï¼‰æ¶‰åŠåˆ°çš„ä¸€äº›å‡½æ•°
- å‡å®šç½‘ç»œä¸ºå¤šå±‚æ„ŸçŸ¥æœºï¼Œç½‘ç»œè¾“å‡ºä¸º $\hat{y} = \text{MLP}(X_{\text{train}})$ï¼Œå…¶ä¸­ $X$ ä¸ºæˆ¿å­çš„ç‰¹å¾ï¼Œ $\text{MLP}$ ä¸ºå¤šå±‚ç¥žç»ç½‘ç»œï¼Œ $W$ å’Œ $b$ ä¸º $\text{MLP}$ çš„å‚æ•°ï¼Œ $\hat{y}$ ä¸ºé¢„æµ‹çš„æˆ¿ä»·ã€‚
- æ¿€æ´»å‡½æ•°é€‰æ‹©ä¸º $ReLU$ ï¼Œæ¿€æ´»å‡½æ•°ä¸ºï¼š
```math
\text{ReLU}(x) = \max(0, x)
```
  - å…¶å¯¼æ•°ä¸ºï¼š
```math
\text{ReLU}'(x) = \begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
```
- è®¾ç½®æŸå¤±å‡½æ•°ä¸º $L_{\text{MSE}}$
```math
L_{\text{MSE}} = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
```
-  $He$ æ–¹æ³•åˆå§‹åŒ–å‚æ•°ä¸ºï¼š
    - å…¶ä¸­ $n_{\text{in}}$ ä¸ºè¾“å…¥å±‚çš„ç¥žç»å…ƒä¸ªæ•°
    - $W$ æœä»Žä¸€ä¸ªå‡å€¼ä¸º $0$ï¼Œæ–¹å·®ä¸º $\frac{2}{n_{\text{in}}}$ çš„æ­£æ€åˆ†å¸ƒï¼Œå¯ä»¥ä¿è¯å€¼åœ¨ç½‘ç»œä¸­ä¸ä¼šè¢«æ”¾å¤§æˆ–ç¼©å°å¤ªå¿«ã€‚
    - $b$ ä¸€å¼€å§‹**ä¸€èˆ¬è®¾ç½®ä¸º $0$**ï¼Œåœ¨è®­ç»ƒä¸­ï¼Œ $b$ çš„å€¼å¯ä»¥å¾ˆå¿«è®­ç»ƒå‡ºæ¥ï¼Œå¦‚æžœè®¾ç½®åƒ $W$ çš„é«˜æ–¯åˆ†å¸ƒï¼Œç›¸å½“äºŽäººä¸ºåŠ äº†â€œéšæœºåç§»â€ï¼Œè¿™äº›åç§»æ—©æœŸåè€Œå¯èƒ½å¹²æ‰°æ¢¯åº¦æ–¹å‘ã€‚
```math
W \sim \mathcal{N}(0, \frac{2}{n_{\text{in}}})
``` 
- ç¥žç»å…ƒçš„è¾“å‡ºä¸ºï¼š
```math
z = W \cdot X + b\\
\hat{y} = \text{ReLU}(W \cdot X + b)
```
- æ‰€ä»¥æŸå¤±å‡½æ•°å¯¹ $W$ çš„å¯¼æ•°ä¸ºï¼š
 ```math 
 \frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial W}
 ```
- æ¢¯åº¦ä¸‹é™æ³•å…¬å¼ï¼š
    ```math
    W = W - \eta \cdot \frac{\partial L}{\partial W}
    ```
#### 2. å…³é”®ä»£ç å±•ç¤º
##### ä»£ç ç»“æž„
```bash
houses_price_pred.py
â”œâ”€â”€ relu(x) - ReLUæ¿€æ´»å‡½æ•°
â”œâ”€â”€ relu_derivative(x) - ReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°
â”œâ”€â”€ load_data(filename) - ä»ŽCSVæ–‡ä»¶ä¸­åŠ è½½æ•°æ®
â”œâ”€â”€ standardScaler(InputData) - æ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ
â”œâ”€â”€ inverseStandardScaler(InputData, mean, std) - åæ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ
â”œâ”€â”€ trainTestSplit(X, Y, test_size=0.2, random_state=None) - åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
â”œâ”€â”€ create_mini_batches(X, Y, batch_size) - mini-batchesåˆ›å»ºå°æ‰¹é‡æ•°æ®
â”œâ”€â”€ MLP
â”‚   â”œâ”€â”€ __init__(self, layers, activation, learning_rate, max_iterations) - åˆå§‹åŒ–å¤šå±‚æ„ŸçŸ¥æœº
â”‚   â”œâ”€â”€ _initialize_parameters(self) - åˆå§‹åŒ–æƒé‡å’Œåç½®
â”‚   â”œâ”€â”€ forward(self, X) - å‰å‘ä¼ æ’­
â”‚   â”œâ”€â”€ backward(self, X, Y, cache) - åå‘ä¼ æ’­
â”‚   â”œâ”€â”€ update(self, gradients) - æ›´æ–°æƒé‡å’Œåç½®
â”‚   â”œâ”€â”€ compute_loss(self, Y, Y_pred) - è®¡ç®—å‡æ–¹è¯¯å·®æŸå¤±
â”‚   â””â”€â”€ run(self, X, Y) - è®­ç»ƒæ¨¡åž‹
â”œâ”€â”€ main() - ä¸»å‡½æ•°
â””â”€â”€ if __name__ == "__main__": - ç¨‹åºå…¥å£
```
##### å…³é”®å‡½æ•°
- æ ‡å‡†åŒ–ä¸Žåæ ‡å‡†åŒ–å‡½æ•°ï¼ˆ`standardScaler` å’Œ `inverseStandardScaler`ï¼‰
    ```py
    # çŸ©é˜µæ ‡å‡†åŒ–
    def standardScaler(InputData):
        """
        æ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ
        :param InputData: ç‰¹å¾çŸ©é˜µ
        :return: å‡å€¼ï¼Œæ ‡å‡†å·®ï¼Œæ ‡å‡†åŒ–åŽçš„ç‰¹å¾çŸ©é˜µ
        """
        mean = np.mean(InputData, axis=0)  # å‡å€¼
        std = np.std(InputData, axis=0)  # æ ‡å‡†å·®
        return (mean, std, (InputData - mean) / std)

    # åæ ‡å‡†åŒ–
    def inverseStandardScaler(InputData, mean, std):
        """
        åæ ‡å‡†åŒ–ç‰¹å¾çŸ©é˜µ
        :param InputData: ç‰¹å¾çŸ©é˜µ
        :param mean: å‡å€¼
        :param std: æ ‡å‡†å·®
        :return: åæ ‡å‡†åŒ–åŽçš„ç‰¹å¾çŸ©é˜µ
        """
        return InputData * std + mean
    ```
  - æ ‡å‡†åŒ–æ˜¯å°†æ•°æ®è½¬æ¢ä¸ºå‡å€¼ä¸º $0$ ï¼Œæ–¹å·®ä¸º $1$ çš„åˆ†å¸ƒï¼Œä½¿å¾—æ•°æ®åœ¨åŒä¸€å°ºåº¦ä¸Šè¿›è¡Œæ¯”è¾ƒã€‚
    - æ ‡å‡†åŒ–å¯ä»¥é¿å…ç‰¹å¾å€¼èŒƒå›´å·®å¼‚å¯¹æ¨¡åž‹è®­ç»ƒçš„å½±å“ï¼Œä¿è¯æ¢¯åº¦æ›´æ–°ç¨³å®šæ€§
  - åæ ‡å‡†åŒ–æ˜¯å°†æ ‡å‡†åŒ–åŽçš„æ•°æ®è½¬æ¢å›žåŽŸå§‹æ•°æ®çš„å°ºåº¦ã€‚
  - ä½¿ç”¨`numpy`åº“ä¸­çš„`mean`å’Œ`std`å‡½æ•°è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œä½¿ç”¨`axis=0`å‚æ•°è¡¨ç¤ºæŒ‰åˆ—è®¡ç®—ã€‚ï¼ˆä¼ å…¥çŸ©é˜µåˆ—è¡¨ç¤ºç‰¹å¾ï¼Œè¡Œè¡¨ç¤ºæ ·æœ¬ï¼‰
- åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å‡½æ•°ï¼ˆ`trainTestSplit`ï¼‰
    ```py
    def trainTestSplit(X, Y, test_size=0.2, random_state=None):
        """
        åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        :param X: ç‰¹å¾çŸ©é˜µ
        :param Y: ç›®æ ‡å˜é‡
        :param test_size: æµ‹è¯•é›†æ¯”ä¾‹
        :param random_state: éšæœºç§å­
        :return: X_train, X_test, Y_train, Y_test
        """
        if random_state is not None:
            np.random.seed(random_state)
        
        # èŽ·å–æ ·æœ¬æ•°é‡
        n_samples = X.shape[0]
        
        # è®¡ç®—æµ‹è¯•é›†æ ·æœ¬æ•°
        n_test = int(n_samples * test_size)
        
        # ç”Ÿæˆéšæœºç´¢å¼•
        indices = np.random.permutation(n_samples)
        test_indices = indices[:n_test]
        train_indices = indices[n_test:]
        
        # åˆ’åˆ†æ•°æ®é›†
        X_train, X_test = X[train_indices], X[test_indices]
        Y_train, Y_test = Y[train_indices], Y[test_indices]
        
        return X_train, X_test, Y_train, Y_test
    ```
  - åˆ’åˆ†æ•°æ®é›†æœ‰åˆ©äºŽè¯„ä¼°æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚
  - ä½¿ç”¨`numpy`åº“ä¸­çš„`random.permutation`å‡½æ•°ç”Ÿæˆéšæœºç´¢å¼•ï¼Œä½¿ç”¨åˆ‡ç‰‡åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
- `create_mini_batches`å‡½æ•°
    ```py
    # mini-batchå‡½æ•°
    def create_mini_batches(X, Y, batch_size):
        """
        mini-batch
        :param X: ç‰¹å¾çŸ©é˜µ
        :param Y: ç›®æ ‡å˜é‡
        :param batch_size: batchå¤§å°
        :return: mini-batchåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(X_batch, Y_batch)
        """
        mini_batches = []
        n_samples = X.shape[0]
        
        # æ´—ç‰Œ
        indices = np.random.permutation(n_samples)
        X_shuffled = X[indices]
        Y_shuffled = Y[indices]
        
        # åˆ›å»ºmini-batches
        num_complete_batches = n_samples // batch_size
        for i in range(num_complete_batches):
            X_batch = X_shuffled[i * batch_size:(i + 1) * batch_size]
            Y_batch = Y_shuffled[i * batch_size:(i + 1) * batch_size]
            mini_batches.append((X_batch, Y_batch))
        
        # å¤„ç†å‰©ä½™æ ·æœ¬
        if n_samples % batch_size != 0:
            X_batch = X_shuffled[num_complete_batches * batch_size:]
            Y_batch = Y_shuffled[num_complete_batches * batch_size:]
            mini_batches.append((X_batch, Y_batch))
        
        return mini_batches
    ```
    - `create_mini_batches`å‡½æ•°ç”¨äºŽåˆ›å»ºå°æ‰¹é‡æ•°æ®ï¼Œè€Œä¸æ˜¯æ‰€æœ‰æ•°æ®ä¸€èµ·è®­ç»ƒ
    - `batch_size`æ˜¯æ¯ä¸ªå°æ‰¹é‡çš„å¤§å°ï¼Œ`mini_batches`æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä¸€ä¸ªå°æ‰¹é‡çš„ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å˜é‡
    - å…ˆéšæœºæ‰“ä¹±æ•°æ®ä½¿å¾—æ›´åŠ æŽ¥è¿‘çœŸå®žæ•°æ®åˆ†å¸ƒï¼Œç„¶åŽåˆ’åˆ†æˆå°æ‰¹é‡
- åˆå§‹åŒ–æƒé‡å’Œåç½®å‡½æ•°ï¼ˆ`_initialize_parameters`ï¼‰
    ```py
    def _initialize_parameters(self):
        """
        åˆå§‹åŒ–æƒé‡å’Œåç½®
        Heåˆå§‹
        """
        for i in range(1, len(self.layers)):
            # æƒé‡çŸ©é˜µåˆå§‹åŒ–ä¸ºéšæœºå€¼ï¼Œåç½®åˆå§‹åŒ–ä¸ºé›¶
            self.weights[i] = np.random.randn(self.layers[i], self.layers[i-1]) * np.sqrt(2 / self.layers[i-1])
            self.biases[i] = np.zeros((self.layers[i], 1))
    ```
    - `np.random.randn(a, b)`ç”Ÿæˆä¸€ä¸ª shape ä¸º (a, b) çš„æ•°ç»„ï¼Œé‡Œé¢æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆå‡å€¼ä¸º $0$ ï¼Œæ–¹å·®ä¸º $1$ ï¼‰éšæœºæ•°ã€‚
    - `np.sqrt(2 / self.layers[i-1])`æŠŠè¿™äº›æ•°æ”¾ç¼©ï¼Œä½¿å¾—å®ƒä»¬å˜æˆæ–¹å·®ä¸º $\frac{2}{n_{\text{in}}}$ çš„æ­£æ€åˆ†å¸ƒ
    - `np.zeros((self.layers[i], 1))`ç”Ÿæˆä¸€ä¸ª shape ä¸º (a, 1) çš„æ•°ç»„ï¼Œé‡Œé¢æ˜¯ $0$ ï¼Œè¡¨ç¤ºåç½®åˆå§‹åŒ–ä¸º $0$ã€‚
- å‰å‘ä¼ æ’­å‡½æ•°ï¼ˆ`forward`ï¼‰
    ```py
    def forward(self, X):
        """
        å‰å‘ä¼ æ’­
        :param X: è¾“å…¥æ•°æ®
        :return: è¾“å‡ºoutå’Œç¼“å­˜cache
        """
        cache = {}  # ç¼“å­˜å­—å…¸ï¼Œè®°å½•æ¯å±‚çš„æƒå’Œå’Œæ¿€æ´»å€¼
        out = X.T # XåŽŸæ¥æ˜¯[8000:4]ï¼Œè½¬ç½®åŽæ˜¯[4:8000]

        for i in range(1, len(self.layers)):
            net = np.dot(self.weights[i], out) + self.biases[i]
            cache['net' + str(i)] = net  # è®°å½•æ¯å±‚çš„æƒå’Œ

            # æŽ’é™¤è¾“å‡ºå±‚
            if i == len(self.layers) - 1:
                out = net
            else:
                out = self.activation(net)
            cache['out' + str(i)] = out

        return out, cache # è¿”å›žçš„outï¼š[1:8000]
    ```
    - è¿™é‡Œéœ€è¦æ³¨æ„çš„ç‚¹æ˜¯ï¼šåœ¨å‰å‘ä¼ æ’­ä¸­åˆ©ç”¨`cache`å­—å…¸è®°å½•æ¯å±‚çš„æƒå’Œå’Œæ¿€æ´»å€¼ï¼ŒåŽç»­åå‘ä¼ æ’­å°±ä¸ç”¨äºŒæ¬¡è®¡ç®—
    - ä½¿ç”¨`np.dot`å‡½æ•°è®¡ç®—çŸ©é˜µä¹˜æ³•ï¼Œè®¡ç®—å‡ºçš„çŸ©é˜µè¡Œä¸ºå½“å‰å±‚çš„ç¥žç»å…ƒä¸ªæ•°ï¼Œåˆ—ä¸ºè®­ç»ƒæ ·æœ¬æ•°
    - è¾“å‡ºå±‚ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œç›´æŽ¥è¾“å‡ºçº¿æ€§å€¼
- åå‘ä¼ æ’­å‡½æ•°ï¼ˆ`backward`ï¼‰
    ```py
    def backward(self, X, Y, cache):
        """
        åå‘ä¼ æ’­
        :param X: è¾“å…¥æ•°æ®
        :param Y: ç›®æ ‡å€¼
        :param cache: å‰å‘ä¼ æ’­çš„ç¼“å­˜
        :return: æ¢¯åº¦
        """
        results = {}
        L = len(self.layers)
        m = X.shape[0] # æ ·æœ¬æ•°é‡
        
        # è¾“å‡ºå±‚æ¢¯åº¦
        dout = (cache['out' + str(L-1)] - Y.T) * (2/m)
        dW = np.dot(dout, cache['out' + str(L-2)].T) if L > 2 else 1/m * np.dot(dout, X)
        db = np.sum(dout, axis=1, keepdims=True)
        
        results[L - 1] = (dW, db) # æƒé‡å’Œåç½®çš„æ¢¯åº¦
        
        # éšè—å±‚æ¢¯åº¦
        for i in reversed(range(1, L-1)):
            dout = np.dot(self.weights[i+1].T, dout)  * self.activation_derivative(cache['net' + str(i)])
            out_prev = cache['out' + str(i-1)] if i > 1 else X.T
            dW = np.dot(dout, out_prev.T)
            db = np.sum(dout, axis=1, keepdims=True)
            
            results[i] = (dW, db)
            
        return results
    ```
- å†æ¥çœ‹å¯¹ $W$ æ±‚å¯¼çš„åå‘ä¼ æ’­å…¬å¼ï¼š
      
```math 
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial W}
``` 

- é‚£ä¹ˆåœ¨çŸ©é˜µè¿ç®—ä¸­ï¼Œå¯ä»¥å†™æˆï¼š(è¿™é‡Œçš„ $X$ ä»£è¡¨çš„æ˜¯å‰ä¸€å±‚çš„è¾“å‡ºï¼Œ $z$ æ˜¯å½“å‰å±‚çš„çº¿æ€§ç»„åˆï¼ˆåŠ æƒå’Œä¸Žåç½®ï¼‰)
    
$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial W}\\
\space \\
= \frac{\partial L}{\partial \hat{y}} \cdot \text{ReLU}'(z) \cdot X
$$

- å¯¹ $b$ çš„æ±‚å¯¼å…¬å¼ä¸ºï¼š

$$
\frac{\partial L}{\partial b} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial b}\\
\space \\
= \frac{\partial L}{\partial \hat{y}} \cdot \text{ReLU}'(z) \cdot 1
$$

- åœ¨ä»£ç å®žé™…è®¡ç®—ä¸­ï¼Œè¾“å‡ºå±‚é‡Œçš„ $dW$ è®¡ç®—æ–¹å¼å’Œéšè—å±‚ä¸ä¸€æ ·ï¼š
  - è¾“å‡ºå±‚ç”±äºŽçº¿æ€§è¾“å‡ºï¼Œæ‰€ä»¥åªè¦è®¡ç®—æŸå¤±å‡½æ•°çš„å¯¼æ•° $\frac{\partial L}{\partial z}$ ï¼Œå³  `dout = (cache['out' + str(L-1)] - Y.T) * (2/m)` 
  - éšè—å±‚éœ€è¦è®¡ç®—æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ï¼Œæ‰€ä»¥è¦ä¹˜ä¸Š `self.activation_derivative(cache['net' + str(i)])`ï¼Œå³ `dout = np.dot(self.weights[i+1].T, dout)  * self.activation_derivative(cache['net' + str(i)])`
    - è¿™é‡Œæ‹¬å·é‡Œé¢çš„ `dout` æ˜¯å³è¾¹ä¸€å±‚çš„`out`æ±‚å¯¼å€¼ï¼Œè®¡ç®—å‡ºæ¥çš„`dout`æ˜¯å½“å‰å±‚çš„`out`æ±‚å¯¼å€¼
    ![alt text](images/image.png)
- è‡³äºŽ $db$ ä¸ºä»€ä¹ˆæ˜¯ `np.sum(dout, axis=1, keepdims=True)`ï¼Œ`dout` çš„åˆ—æ˜¯æ ·æœ¬æ•°ï¼Œè¡Œæ˜¯ç¥žç»å…ƒä¸ªæ•°ï¼Œ`np.sum(dout, axis=1, keepdims=True)` å°±æ˜¯å¯¹æ¯ä¸€è¡Œæ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸ªç¥žç»å…ƒçš„åç½®æ¢¯åº¦ï¼Œå› ä¸ºæ¯ä¸€ä¸ªæ ·æœ¬éƒ½ä¼šå¯¹æ¯ä¸€ä¸ªç¥žç»å…ƒçš„åç½®äº§ç”Ÿå½±å“
- `main` å‡½æ•°
    ```py
    def main():
        X,Y = load_data("MLP_data.csv")

        # æ•°æ®æ ‡å‡†åŒ–
        _,_,std_X_matrix = standardScaler(X)
        mean,std_Y,std_Y_matrix = standardScaler(Y)

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, Y_train, Y_test = trainTestSplit(std_X_matrix, std_Y_matrix, test_size=0.2, random_state=42)

        # åˆ›å»ºMLPæ¨¡åž‹
        mlp = MLP(layers=[X.shape[1], 32, 16, 8, 1], activation='relu', learning_rate=0.025, max_iterations=1000)
        mlp.run(X_train, Y_train)

        # é¢„æµ‹å’Œåæ ‡å‡†åŒ–ï¼ˆå°†æµ‹è¯•é›†æ•°æ®è¾“å…¥ï¼‰
        Y_pred, _ = mlp.forward(X_test)
        Y_pred = inverseStandardScaler(Y_pred.T, mean, std_Y)  # åæ ‡å‡†åŒ–ï¼Œæ³¨æ„è½¬ç½®
        
        Y_original = inverseStandardScaler(Y_test, mean, std_Y)  # åæ ‡å‡†åŒ–

        # è®¡ç®—å‡æ–¹è¯¯å·®
        mse = np.mean((Y_pred - Y_original) ** 2)
        print(f"æµ‹è¯•é›†MSE: {mse}")

        # ...ï¼ˆå¯è§†åŒ–éƒ¨åˆ†çœç•¥ï¼‰
    ```
    - æŒ‰ç…§æµç¨‹å°†å„ä¸ªå‡½æ•°ä¸²èµ·æ¥å³å¯ï¼š
      - åŠ è½½æ•°æ®
      - æ ‡å‡†åŒ–æ•°æ®
      - åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
      - åˆ›å»ºæ¨¡åž‹
      - è®­ç»ƒæ¨¡åž‹
      - é¢„æµ‹
      - åæ ‡å‡†åŒ–
      - è®¡ç®—å‡æ–¹è¯¯å·®
- å¯è§†åŒ–éƒ¨åˆ†
  - ç»˜åˆ¶ loss æ›²çº¿ä»¥åŠé¢„æµ‹ç»“æžœä¸ŽçœŸå®žå€¼çš„å¯¹æ¯”å›¾
    ```py
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(range(len(mlp.losses)), mlp.losses, label='Loss Curve')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.title('Training Loss Curve')
    plt.legend()
    plt.grid(True)
    plt.savefig('loss_curve.svg')
    plt.show()

    # ç»˜åˆ¶é¢„æµ‹å€¼ä¸ŽçœŸå®žå€¼çš„æ•£ç‚¹å›¾
    plt.figure(figsize=(10, 6))
    plt.scatter(Y_original, Y_pred, alpha=0.5)
    plt.plot([Y_original.min(), Y_original.max()], 
                [Y_original.min(), Y_original.max()], 
                'r--', lw=2)
    plt.xlabel('Actual Price')
    plt.ylabel('Predicted Price')
    plt.title('Actual vs Predicted House Prices')
    plt.savefig('prediction_results.svg')
    plt.grid(True)
    plt.show()
    ```
  - ä½¿ç”¨ `pca` è¿›è¡Œé™ç»´ï¼ˆ4 to 2ï¼‰
      ```py
      from sklearn.decomposition import PCA
      # ä½¿ç”¨PCAå°†ç‰¹å¾ä»Ž4ç»´é™è‡³2ç»´
      pca = PCA(n_components=2)
      X_test_original = inverseStandardScaler(X_test, *standardScaler(X)[:2])
      X_pca = pca.fit_transform(X_test_original)
      ```
  - ç»˜åˆ¶é™ç»´åŽçš„ 3D æ•£ç‚¹å›¾
    - ä»¥é™ç»´åŽçš„ç‰¹å¾ä½œä¸º x,y è½´ï¼ŒçœŸå®žæˆ¿ä»·ä½œä¸º z è½´
    ```py
    # 3Dæ•£ç‚¹å›¾ï¼šé™ç»´ç‰¹å¾ä½œä¸ºx,yè½´ï¼ŒçœŸå®žæˆ¿ä»·ä½œä¸ºzè½´
    fig = plt.figure(figsize=(12, 10))
    ax1 = fig.add_subplot(111, projection='3d')
    scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], Y_original.flatten(), 
                         c=Y_original.flatten(), cmap='viridis', s=50, alpha=0.6)
    ax1.set_xlabel('PCA Feature 1')
    ax1.set_ylabel('PCA Feature 2')
    ax1.set_zlabel('Actual House Price')
    ax1.set_title('3D Scatter Plot of Actual House Prices')
    plt.colorbar(scatter, ax=ax1, label='House Price')
    plt.show()
    ```
  - ç»˜åˆ¶é™ç»´åŽçš„ 3D æ›²é¢å›¾
    ```py
    # 3Dæ›²é¢å›¾ï¼šé™ç»´ç‰¹å¾ä½œä¸ºx,yè½´ï¼Œé¢„æµ‹æˆ¿ä»·ä½œä¸ºzè½´
    fig = plt.figure(figsize=(12, 10))
    ax2 = fig.add_subplot(111, projection='3d')

    # åˆ›å»ºç½‘æ ¼ä»¥ä¾¿ç»˜åˆ¶å¹³æ»‘æ›²é¢
    xi = np.linspace(min(X_pca[:, 0]), max(X_pca[:, 0]), 100)
    yi = np.linspace(min(X_pca[:, 1]), max(X_pca[:, 1]), 100)
    X1, Y1 = np.meshgrid(xi, yi)

    # æ’å€¼å¾—åˆ°å¹³æ»‘çš„zå€¼
    Z = griddata((X_pca[:, 0], X_pca[:, 1]), Y_pred.flatten(), 
                (X1, Y1), method='cubic', fill_value=Y_pred.mean())

    # ç»˜åˆ¶æ›²é¢
    surf = ax2.plot_surface(X1, Y1, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True, alpha=0.8)

    # æ·»åŠ åŽŸå§‹æ•£ç‚¹
    ax2.scatter(X_pca[:, 0], X_pca[:, 1], Y_pred.flatten(), c='black', s=10, alpha=0.5)

    ax2.set_xlabel('PCA Feature 1')
    ax2.set_ylabel('PCA Feature 2')
    ax2.set_zlabel('Predicted House Price')
    ax2.set_title('3D Surface Plot of Predicted House Prices')
    plt.colorbar(surf, ax=ax2, label='Predicted House Price')
    plt.show()
    ```
    - `griddata`å‡½æ•°ç”¨äºŽæ’å€¼ï¼Œåˆ›å»ºä¸€ä¸ªç½‘æ ¼ä»¥ä¾¿ç»˜åˆ¶å¹³æ»‘æ›²é¢
    - `plot_surface`å‡½æ•°ç”¨äºŽç»˜åˆ¶æ›²é¢å›¾ï¼Œ`scatter`å‡½æ•°ç”¨äºŽç»˜åˆ¶æ•£ç‚¹å›¾
  - æŸ¥çœ‹åŽŸå§‹ç‰¹å¾çš„è´¡çŒ®
    - ç”±äºŽä»Ž 4 ç»´é™åˆ° 2 ç»´ï¼Œç»˜åˆ¶é™ç»´åŽçš„ç‰¹å¾è´¡çŒ®å›¾æœ‰åˆ©äºŽåˆ†æžåŽŸå§‹ç‰¹å¾å¯¹é¢„æµ‹ç»“æžœçš„å½±å“
    ```py
    # æŸ¥çœ‹åŽŸå§‹ç‰¹å¾å¯¹ä¸»æˆåˆ†çš„è´¡çŒ®
    components = pd.DataFrame(
        pca.components_,
        columns=['ç»åº¦', 'çº¬åº¦', 'æˆ¿é¾„', 'æˆ¿ä¸»æ”¶å…¥']
    )
    plt.figure(figsize=(10, 6))
    plt.imshow(components, cmap='coolwarm')
    plt.xticks(range(4), ['ç»åº¦', 'çº¬åº¦', 'æˆ¿é¾„', 'æˆ¿ä¸»æ”¶å…¥'])
    plt.yticks(range(2), ['PC1', 'PC2'])
    plt.colorbar()
    plt.title('PCA ç»„æˆçƒ­åŠ›å›¾')
    plt.show()
    ```
#### 3. åˆ›æ–°ç‚¹&ä¼˜åŒ–
- é‡‡ç”¨çŸ©é˜µè¿ç®—åŠ é€Ÿè®¡ç®—
  - ä½¿ç”¨`numpy`åº“è¿›è¡ŒçŸ©é˜µè¿ç®—ï¼Œé¿å…äº†å¾ªçŽ¯ï¼Œæé«˜äº†è®¡ç®—æ•ˆçŽ‡
- åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
  - åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¯ä»¥è¯„ä¼°æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé¿å…è¿‡æ‹Ÿåˆ
- å¯¹åˆå§‹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–
  - æ ‡å‡†åŒ–æ•°æ®å¯ä»¥é¿å…ç‰¹å¾å€¼èŒƒå›´å·®å¼‚å¯¹æ¨¡åž‹è®­ç»ƒçš„å½±å“ï¼Œä¿è¯æ¢¯åº¦æ›´æ–°ç¨³å®šæ€§
- å¯¹åˆå§‹æƒé‡å’Œåç½®é‡‡ç”¨`He`åˆå§‹åŒ–
  - `He`åˆå§‹åŒ–å¯ä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„é—®é¢˜ï¼Œä½¿å¾—æ¨¡åž‹æ›´å®¹æ˜“æ”¶æ•›
- ä½¿ç”¨`mini-batch`è®­ç»ƒ
  - `mini-batch`è®­ç»ƒæ•ˆæžœæ˜¾è‘—ï¼Œç›¸æ¯”äºŽå…¨é‡è®­ç»ƒï¼Œloss æ•°å€¼æ›´å°ï¼Œç»“æžœæ›´ä¼˜ç§€ã€å¯¹æ¯”è§ â†“[ä¸‰.å®žéªŒç»“æžœåŠåˆ†æž](#ä¸‰å®žéªŒç»“æžœåŠåˆ†æž)ã€‘
### ä¸‰.å®žéªŒç»“æžœåŠåˆ†æž
> - 1. å¯¹æ¯”ä¸åŒæ„ŸçŸ¥æœºå±‚æ•°ï¼Œå­¦ä¹ çŽ‡çš„ç»“æžœã€è¿­ä»£æ¬¡æ•°é»˜è®¤ä¸º 2000ã€‘
> - 2. å¯¹æ¯”`mini-batch`å’Œå…¨é‡è®­ç»ƒçš„ç»“æžœ
> - å…¶ä»–é»˜è®¤å‚æ•°ï¼š
>   - æ¿€æ´»å‡½æ•°ï¼š`ReLU`
>   - æŸå¤±å‡½æ•°ï¼š`MSE`
>   - è®­ç»ƒé›†æ¯”ä¾‹ï¼š`0.8`
>   - æ‰¹é‡å¤§å°ï¼š`64`
#### 1. å¯¹æ¯”ä¸åŒæ„ŸçŸ¥æœºå±‚æ•°ï¼Œå­¦ä¹ çŽ‡çš„ç»“æžœï¼ˆä½¿ç”¨`mini-batch`è®­ç»ƒï¼‰
|  -/-  |        æ„ŸçŸ¥æœºå±‚æ•°è®¾ç½®        | å­¦ä¹ çŽ‡ | æœ€ç»ˆ loss  |
| :---: | :--------------------------: | :----: | :---: |
|   **1**   | `[X.shape[1], 32, 16, 8, 1]` | 0.025  | 0.16043499676456366 |
|   **2**   | `[X.shape[1], 32, 16, 8, 1]` |  0.01  | 0.17424171263326405 |
|   **3**   |  `[X.shape[1], 64, 32, 1]`   | 0.025  | 0.15377667997212527 |
|   **4**   |  `[X.shape[1], 64, 32, 1]`   | 0.01  | 0.18376833214706986 |
##### å®žéªŒç¼–å· 1
- loss æ›²çº¿å›¾
  ![](./images/loss_curve1_1.svg)
- å®žé™…å€¼ä¸Žé¢„æµ‹å€¼å¯¹æ¯”å›¾
  ![](./images/prediction_results1_1.svg)
- 3D å®žé™…å€¼çš„æ•£ç‚¹å›¾ã€htmlæ–‡ä»¶ï¼Œç‚¹å‡»æŸ¥çœ‹ã€‘
    ![](./images/p_a_1_1.png)
  - [3D_scatter_actual_price](./images/3D_scatter_actual_price1_1.html)
- 3D æ›²é¢å›¾ ã€htmlæ–‡ä»¶ï¼Œç‚¹å‡»æŸ¥çœ‹ã€‘
  ![](./images/p_p_1_1.png)
  - [3D_surface_predicted_price](./images/3D_surface_predicted_price1_1.html)
##### å®žéªŒç¼–å· 2
- loss æ›²çº¿å›¾
  ![](./images/loss_curve1_2.svg)
- å®žé™…å€¼ä¸Žé¢„æµ‹å€¼å¯¹æ¯”å›¾
  ![](./images/prediction_results1_2.svg)
- 3D å®žé™…å€¼çš„æ•£ç‚¹å›¾ã€htmlæ–‡ä»¶ï¼Œç‚¹å‡»æŸ¥çœ‹ã€‘
    ![](./images/p_a_1_2.png)
  - [3D_scatter_actual_price](./images/3D_scatter_actual_price1_2.html)
- 3D æ›²é¢å›¾ ã€htmlæ–‡ä»¶ï¼Œç‚¹å‡»æŸ¥çœ‹ã€‘
    ![](./images/p_p_1_2.png)
  - [3D_surface_predicted_price](./images/3D_surface_predicted_price1_2.html)
##### å®žéªŒç¼–å· 3
- loss æ›²çº¿å›¾
  ![](./images/loss_curve1_3.svg)
- å®žé™…å€¼ä¸Žé¢„æµ‹å€¼å¯¹æ¯”å›¾
    ![](./images/prediction_results1_3.svg)
- 3D å®žé™…å€¼çš„æ•£ç‚¹å›¾ã€htmlæ–‡ä»¶ï¼Œç‚¹å‡»æŸ¥çœ‹ã€‘
    ![](./images/p_a_1_3.png)
  - [3D_scatter_actual_price](./images/3D_scatter_actual_price1_3.html)
- 3D æ›²é¢å›¾ ã€htmlæ–‡ä»¶ï¼Œç‚¹å‡»æŸ¥çœ‹ã€‘
    ![](./images/p_p_1_3.png)
  - [3D_surface_predicted_price](./images/3D_surface_predicted_price1_3.html)
##### å®žéªŒç¼–å· 4
- loss æ›²çº¿å›¾
  ![](./images/loss_curve1_4.svg)
- å®žé™…å€¼ä¸Žé¢„æµ‹å€¼å¯¹æ¯”å›¾
  ![](./images/prediction_results1_4.svg)
- 3D å®žé™…å€¼çš„æ•£ç‚¹å›¾ã€htmlæ–‡ä»¶ï¼Œç‚¹å‡»æŸ¥çœ‹ã€‘
    ![](./images/p_a_1_4.png)
  - [3D_scatter_actual_price](./images/3D_scatter_actual_price1_4.html)
- 3D æ›²é¢å›¾ ã€htmlæ–‡ä»¶ï¼Œç‚¹å‡»æŸ¥çœ‹ã€‘
    ![](./images/p_p_1_4.png)
  - [3D_surface_predicted_price](./images/3D_surface_predicted_price1_4.html)
##### å¯¹æ¯”æ€»ç»“
- æ„ŸçŸ¥æœºå±‚æ•°å¤šï¼Œåˆ™å­¦ä¹ çŽ‡å¯¹ç»“æžœçš„å½±å“è¾ƒå°
- æ„ŸçŸ¥æœºå±‚æ•°å°‘ï¼Œåˆ™å­¦ä¹ çŽ‡å¯¹ç»“æžœçš„å½±å“è¾ƒå¤§
- å­¦ä¹ çŽ‡è¿‡å¤§ï¼Œloss æ›²çº¿éœ‡è¡
##### é™„ï¼š
- pca ç»„æˆçƒ­åŠ›å›¾ï¼š
    ![](./images/pca_components.svg)
#### 2. å¯¹æ¯”`mini-batch`å’Œå…¨é‡è®­ç»ƒçš„ç»“æžœ
- é€‰å–å®žéªŒç¼–å· 1 å’Œå®žéªŒç¼–å· 3 çš„ç»“æžœè¿›è¡Œå¯¹æ¯”
##### å®žéªŒç¼–å· 1
###### ä¸ä½¿ç”¨`mini-batch`è®­ç»ƒ
- è¿­ä»£ 2000 ä»£åŽ loss çš„å€¼ä¸º  $0.3164235086662772$ 
  - loss æ›²çº¿å›¾
    ![](./images/loss_curve2_1.svg)
  - å®žé™…å€¼ä¸Žé¢„æµ‹å€¼å¯¹æ¯”å›¾
    ![](./images/prediction_results2_1.svg)
##### å®žéªŒç¼–å· 3
###### ä¸ä½¿ç”¨`mini-batch`è®­ç»ƒ
- è¿­ä»£ 2000 ä»£åŽ loss çš„å€¼ä¸º  $0.3159459963328876$ 
  - loss æ›²çº¿å›¾
    ![](./images/loss_curve2_3.svg)
  - å®žé™…å€¼ä¸Žé¢„æµ‹å€¼å¯¹æ¯”å›¾
    ![](./images/prediction_results2_3.svg)
##### æ€»ç»“
- ä½¿ç”¨`mini-batch`è®­ç»ƒçš„ç»“æžœæ›´å¥½ï¼Œloss æ•°å€¼æ›´å°ï¼Œç»“æžœæ›´ä¼˜ç§€
- ä½¿ç”¨å…¨é‡è®­ç»ƒä¼šå‡ºçŽ°è¾ƒå¤§çš„æ³¢åŠ¨ï¼Œè€Œä½¿ç”¨`mini-batch` æ³¢åŠ¨è¾ƒå°
### å››.å‚è€ƒèµ„æ–™
> - https://plotly.com/python/pca-visualization/
> - https://blog.csdn.net/HLBoy_happy/article/details/77146012?fromshare=blogdetail&sharetype=blogdetail&sharerId=77146012&sharerefer=PC&sharesource=MLLeslie&sharefrom=from_link
