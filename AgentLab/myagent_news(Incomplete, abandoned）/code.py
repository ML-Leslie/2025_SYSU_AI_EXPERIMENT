import os
from typing import List, TypedDict

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_google_genai import ChatGoogleGenerativeAI
from tavily import TavilyClient

from langgraph.graph import StateGraph, END

os.environ["GOOGLE_API_KEY"] = ""
os.environ['http_proxy'] = 'http://127.0.0.1:7897'
os.environ['https_proxy'] = 'http://127.0.0.1:7897'
os.environ["TAVILY_API_KEY"] = ""

# --- 1. å®šä¹‰å›¾çš„çŠ¶æ€ (State) ---
# Stateæ˜¯æ•´ä¸ªæµç¨‹ä¸­æ•°æ®çš„è½½ä½“ï¼Œå¯ä»¥ç†è§£ä¸ºæ¯ä¸ªAgentå…±äº«çš„â€œçŸ­æœŸè®°å¿†â€ã€‚
class GraphState(TypedDict):
    topic: str  # ç”¨æˆ·è¾“å…¥çš„ä¸»é¢˜
    outline: str  # ç”±ä¸»ç¼–Agentç”Ÿæˆçš„å¤§çº²
    research_data: List[dict]  # ç”±æœç´¢Agentæ”¶é›†çš„èµ„æ–™
    draft: str  # ç”±æ’°ç¨¿äººAgentç”Ÿæˆçš„åˆç¨¿
    reflection: str # ç”±äº‹å®æ ¸æŸ¥Agentç”Ÿæˆçš„åæ€/ä¿®æ”¹å»ºè®®
    final_report: str # æœ€ç»ˆç”Ÿæˆçš„æŠ¥å‘Š

# --- 2. å‡†å¤‡å·¥å…· (Tools) ---
# è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨Tavilyä½œä¸ºæœç´¢å·¥å…·
tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])


# --- 3. åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹ (LLM) ---
# æˆ‘ä»¬å°†ä½¿ç”¨Googleçš„Geminiæ¨¡å‹æ¥é©±åŠ¨æ‰€æœ‰çš„Agent
llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0)


# --- 4. å®šä¹‰Agentçš„è¾“å‡ºç»“æ„ (Pydantic Models) ---
# ä½¿ç”¨Pydanticå¯ä»¥å¼ºåˆ¶LLMè¾“å‡ºæˆ‘ä»¬æƒ³è¦çš„JSONæ ¼å¼ï¼Œæ–¹ä¾¿ç¨‹åºè§£æã€‚

class ReportOutline(BaseModel):
    """æŠ¥å‘Šå¤§çº²çš„ç»“æ„"""
    outline: str = Field(description="æŠ¥å‘Šçš„ç« èŠ‚å¤§çº²ï¼Œä½¿ç”¨Markdownæ ¼å¼ã€‚")

class ResearchInfo(BaseModel):
    """ç ”ç©¶ä¿¡æ¯çš„ç»“æ„"""
    results: List[dict] = Field(description="ä¸€ä¸ªåŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€æ¡æœç´¢ç»“æœï¼ŒåŒ…å«'url'å’Œ'content'ã€‚")

class ReportDraft(BaseModel):
    """æŠ¥å‘Šåˆç¨¿çš„ç»“æ„"""
    draft: str = Field(description="æŠ¥å‘Šçš„å®Œæ•´åˆç¨¿ï¼Œä½¿ç”¨Markdownæ ¼å¼ã€‚")
    
class Reflection(BaseModel):
    """åæ€ä¸ä¿®æ”¹å»ºè®®çš„ç»“æ„"""
    reflection_notes: str = Field(description="å…³äºåˆç¨¿çš„ä¿®æ”¹å»ºè®®ã€‚å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œè¯·è¿”å›'OK'ã€‚")
    is_ok: bool = Field(description="åˆç¨¿æ˜¯å¦é€šè¿‡æ ¸æŸ¥ï¼Œæ— éœ€ä¿®æ”¹ã€‚")


# --- 5. åˆ›å»ºå„ä¸ªAgentçš„èŠ‚ç‚¹ (Nodes) ---

def chief_editor_node(state: GraphState):
    """
    ä¸»ç¼–AgentèŠ‚ç‚¹
    åŠŸèƒ½ï¼šæ ¹æ®ç”¨æˆ·ä¸»é¢˜ï¼Œç”ŸæˆæŠ¥å‘Šå¤§çº²ã€‚
    """
    print(">> è¿›å…¥ [ä¸»ç¼–Agent] èŠ‚ç‚¹")
    prompt = ChatPromptTemplate.from_template(
        """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ–°é—»ä¸»ç¼–ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºä¸€ä¸ªç»™å®šçš„ä¸»é¢˜åˆ›å»ºä¸€ä¸ªç®€æ´ã€å…¨é¢ã€é€»è¾‘æ¸…æ™°çš„æŠ¥å‘Šå¤§çº²ã€‚
        ä¸»é¢˜: {topic}"""
    )
    # .with_structured_output() ä¼šè®©LLMçš„è¾“å‡ºè‡ªåŠ¨æ ¼å¼åŒ–ä¸ºæˆ‘ä»¬å®šä¹‰çš„Pydanticæ¨¡å‹
    chain = prompt | llm.with_structured_output(ReportOutline)
    result = chain.invoke({"topic": state["topic"]})
    
    print(f"   - ç”Ÿæˆçš„å¤§çº²:\n{result.outline}")
    return {"outline": result.outline}

def search_agent_node(state: GraphState):
    """
    æœç´¢AgentèŠ‚ç‚¹
    åŠŸèƒ½ï¼šæ ¹æ®ä¸»é¢˜å’Œå¤§çº²ï¼Œä½¿ç”¨Tavilyæœç´¢ç›¸å…³ä¿¡æ¯ã€‚
    """
    print(">> è¿›å…¥ [æœç´¢Agent] èŠ‚ç‚¹")
    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ä¸»é¢˜è¿›è¡Œæœç´¢ï¼Œæ›´å¤æ‚çš„å®ç°å¯ä»¥ç»“åˆå¤§çº²ç”Ÿæˆæ›´å…·ä½“çš„æœç´¢æŸ¥è¯¢
    search_query = f"å…³äº {state['topic']} çš„æœ€æ–°è¿›å±•å’Œè¯¦ç»†ä¿¡æ¯"
    print(f"   - æ­£åœ¨æ‰§è¡Œæœç´¢: {search_query}")
    
    # Tavilyçš„ `search` æ–¹æ³•è¿”å›ä¸°å¯Œçš„æœç´¢ç»“æœ
    response = tavily_client.search(query=search_query, search_depth="advanced", max_results=5)
    
    # å°†ç»“æœæ ¼å¼åŒ–ä¸ºæˆ‘ä»¬éœ€è¦çš„å­—å…¸åˆ—è¡¨
    research_data = [{"url": res["url"], "content": res["content"]} for res in response["results"]]
    
    print(f"   - æ‰¾åˆ° {len(research_data)} æ¡ç›¸å…³ä¿¡æ¯")
    return {"research_data": research_data}

def writer_agent_node(state: GraphState):
    """
    æ’°ç¨¿äººAgentèŠ‚ç‚¹
    åŠŸèƒ½ï¼šæ ¹æ®å¤§çº²å’Œç ”ç©¶èµ„æ–™æ’°å†™æŠ¥å‘Šåˆç¨¿ã€‚
    """
    print(">> è¿›å…¥ [æ’°ç¨¿äººAgent] èŠ‚ç‚¹")
    
    # å¦‚æœæœ‰åæ€ï¼ˆä¿®æ”¹æ„è§ï¼‰ï¼Œåˆ™éœ€è¦æ ¹æ®ä¿®æ”¹æ„è§æ¥é‡æ–°æ’°å†™
    if state.get("reflection"):
        print("   - æ£€æµ‹åˆ°ä¿®æ”¹æ„è§ï¼Œæ­£åœ¨è¿›è¡Œä¿®è®¢...")
        prompt_template = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»æ’°ç¨¿äººã€‚è¯·æ ¹æ®ä»¥ä¸‹å¤§çº²ã€ç ”ç©¶èµ„æ–™å’Œä¿®æ”¹å»ºè®®ï¼Œé‡æ–°æ’°å†™ä¸€ä»½è¯¦ç»†ã€å®¢è§‚ã€é«˜è´¨é‡çš„æ–°é—»æŠ¥å‘Šã€‚
        
        ã€åŸå§‹å¤§çº²ã€‘
        {outline}
        
        ã€ç ”ç©¶èµ„æ–™ã€‘
        {research_data}
        
        ã€å¿…é¡»éµå®ˆçš„ä¿®æ”¹å»ºè®®ã€‘
        {reflection}
        
        è¯·è¾“å‡ºä¸€ä»½å®Œæ•´çš„ã€ä¿®è®¢åçš„æŠ¥å‘Šã€‚
        """
        prompt = ChatPromptTemplate.from_template(prompt_template)
        chain = prompt | llm.with_structured_output(ReportDraft)
        result = chain.invoke({
            "outline": state["outline"],
            "research_data": state["research_data"],
            "reflection": state["reflection"]
        })
    else:
        # é¦–æ¬¡æ’°å†™
        print("   - æ­£åœ¨æ’°å†™åˆç¨¿...")
        prompt_template = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»æ’°ç¨¿äººã€‚è¯·æ ¹æ®ä»¥ä¸‹å¤§çº²å’Œç ”ç©¶èµ„æ–™ï¼Œæ’°å†™ä¸€ä»½è¯¦ç»†ã€å®¢è§‚ã€é«˜è´¨é‡çš„æ–°é—»æŠ¥å‘Šã€‚
        
        ã€æŠ¥å‘Šå¤§çº²ã€‘
        {outline}
        
        ã€ç ”ç©¶èµ„æ–™ã€‘
        {research_data}
        
        è¯·ç¡®ä¿æŠ¥å‘Šå†…å®¹å®Œå…¨åŸºäºæä¾›çš„ç ”ç©¶èµ„æ–™ï¼Œå¹¶éµå¾ªå¤§çº²ç»“æ„ã€‚
        """
        prompt = ChatPromptTemplate.from_template(prompt_template)
        chain = prompt | llm.with_structured_output(ReportDraft)
        result = chain.invoke({
            "outline": state["outline"],
            "research_data": str(state["research_data"]) # è½¬ä¸ºå­—ç¬¦ä¸²æ–¹ä¾¿å¤„ç†
        })

    print("   - åˆç¨¿/ä¿®è®¢ç¨¿å®Œæˆã€‚")
    return {"draft": result.draft}

def fact_checker_node(state: GraphState):
    """
    äº‹å®æ ¸æŸ¥AgentèŠ‚ç‚¹ (å®ç°Reflection)
    åŠŸèƒ½ï¼šæ ¸æŸ¥åˆç¨¿å†…å®¹æ˜¯å¦ä¸åŸå§‹èµ„æ–™ä¸€è‡´ï¼Œå¹¶æä¾›ä¿®æ”¹å»ºè®®ã€‚
    """
    print(">> è¿›å…¥ [äº‹å®æ ¸æŸ¥Agent & åæ€] èŠ‚ç‚¹")
    prompt = ChatPromptTemplate.from_template(
        """ä½ æ˜¯ä¸€åä¸¥è°¨çš„äº‹å®æ ¸æŸ¥å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»”ç»†é˜…è¯»æŠ¥å‘Šåˆç¨¿ï¼Œå¹¶å°†å…¶ä¸åŸå§‹ç ”ç©¶èµ„æ–™è¿›è¡Œæ¯”å¯¹ã€‚
        æ£€æŸ¥æŠ¥å‘Šä¸­æ˜¯å¦æœ‰ä»»ä½•ä¸å‡†ç¡®ã€å¤¸å¤§æˆ–ç¼ºä¹ä¾æ®çš„é™ˆè¿°ã€‚
        
        - å¦‚æœæŠ¥å‘Šå†…å®¹å‡†ç¡®æ— è¯¯ï¼Œå¿ äºåŸæ–‡ï¼Œè¯·åœ¨ä¿®æ”¹å»ºè®®ä¸­ä»…è¿”å› "OK"ã€‚
        - å¦‚æœå‘ç°é—®é¢˜ï¼Œè¯·æ¸…æ™°åœ°æŒ‡å‡ºé—®é¢˜æ‰€åœ¨ï¼Œå¹¶æå‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®ã€‚
        
        ã€æŠ¥å‘Šåˆç¨¿ã€‘
        {draft}
        
        ã€åŸå§‹ç ”ç©¶èµ„æ–™ã€‘
        {research_data}
        """
    )
    chain = prompt | llm.with_structured_output(Reflection)
    result = chain.invoke({
        "draft": state["draft"],
        "research_data": str(state["research_data"])
    })
    
    if result.is_ok:
        print("   - æ ¸æŸ¥ç»“æœ: [é€šè¿‡]")
        # å¦‚æœé€šè¿‡ï¼Œå°†è‰ç¨¿å®šä¸ºæœ€ç»ˆæŠ¥å‘Š
        return {"final_report": state["draft"], "reflection": ""}
    else:
        print(f"   - æ ¸æŸ¥ç»“æœ: [éœ€è¦ä¿®æ”¹]")
        print(f"   - ä¿®æ”¹å»ºè®®: {result.reflection_notes}")
        # å¦‚æœä¸é€šè¿‡ï¼Œå°†ä¿®æ”¹æ„è§å­˜å…¥stateï¼Œä»¥ä¾¿æ’°ç¨¿äººè¿›è¡Œä¿®æ”¹
        return {"reflection": result.reflection_notes}


# --- 6. å®šä¹‰å›¾çš„é€»è¾‘æµ (Edges) ---

def should_revise_edge(state: GraphState):
    """
    æ¡ä»¶åˆ¤æ–­è¾¹
    åŠŸèƒ½ï¼šæ ¹æ®äº‹å®æ ¸æŸ¥Agentçš„ç»“æœï¼Œå†³å®šæ˜¯è¿”å›ä¿®æ”¹è¿˜æ˜¯ç»“æŸæµç¨‹ã€‚
    """
    print(">> è¿›è¡Œæ¡ä»¶åˆ¤æ–­: æ˜¯å¦éœ€è¦ä¿®è®¢ï¼Ÿ")
    if state.get("reflection"):
        print("   - å†³ç­–: æ˜¯ï¼Œè¿”å›æ’°ç¨¿äººè¿›è¡Œä¿®æ”¹ã€‚")
        return "revise" # è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¯¹åº”æˆ‘ä»¬æ·»åŠ çš„æ¡ä»¶è¾¹çš„åç§°
    else:
        print("   - å†³ç­–: å¦ï¼Œæµç¨‹ç»“æŸã€‚")
        return END


# --- 7. æ„å»ºå¹¶ç¼–è¯‘å›¾ (Graph) ---
# è¿™æ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼Œä½†é€šè¿‡æ¡ä»¶è¾¹å¯ä»¥å®ç°å¾ªç¯
workflow = StateGraph(GraphState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("chief_editor", chief_editor_node)
workflow.add_node("searcher", search_agent_node)
workflow.add_node("writer", writer_agent_node)
workflow.add_node("fact_checker", fact_checker_node)

# è®¾ç½®å…¥å£ç‚¹
workflow.set_entry_point("chief_editor")

# æ·»åŠ è¾¹
workflow.add_edge("chief_editor", "searcher")
workflow.add_edge("searcher", "writer")
workflow.add_edge("writer", "fact_checker")

# æ·»åŠ æ¡ä»¶è¾¹ï¼ˆå®ç°Reflectionå¾ªç¯ï¼‰
workflow.add_conditional_edges(
    "fact_checker",
    should_revise_edge,
    {
        "revise": "writer", # å¦‚æœ`should_revise_edge`è¿”å›"revise"ï¼Œåˆ™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ˜¯`writer`
        END: END            # å¦‚æœè¿”å›ENDï¼Œåˆ™æµç¨‹ç»“æŸ
    }
)

# ç¼–è¯‘å›¾
app = workflow.compile()


# --- 8. è¿è¡Œå›¾å¹¶æŸ¥çœ‹ç»“æœ ---

if __name__ == "__main__":
    # å®šä¹‰ä¸€ä¸ªè¾“å…¥ï¼Œè¿™ä¸ªå­—å…¸çš„keyå¿…é¡»å’ŒGraphStateä¸­çš„keyå¯¹åº”
    inputs = {"topic": "è‹±ä¼Ÿè¾¾Blackwellæ¶æ„çš„æœ€æ–°è¿›å±•"}
    
    # LangGraphçš„ `stream` æ–¹æ³•ä¼šè¿”å›æ¯ä¸€æ­¥çš„ç»“æœï¼Œæ–¹ä¾¿æˆ‘ä»¬è§‚å¯Ÿæµç¨‹
    for output in app.stream(inputs, stream_mode="values"):
        # `output` æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œkeyæ˜¯èŠ‚ç‚¹åï¼Œvalueæ˜¯è¯¥èŠ‚ç‚¹çš„è¿”å›ç»“æœ
        # æˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºæ¥çœ‹çœ‹æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆ
        print("---")
        # print(output) # æ‰“å°åŸå§‹è¾“å‡º
    
    # `stream` ç»“æŸåï¼Œæœ€åä¸€ä¸ª `output` å€¼å°±æ˜¯æ•´ä¸ªå›¾çš„æœ€ç»ˆçŠ¶æ€
    final_state = output
    
    print("\n\nâœ… æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼")
    print("=" * 30)
    print(final_state["final_report"])

    # ç®€å•æ¼”ç¤ºé•¿æœŸè®°å¿†çš„ç†å¿µ
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæŠŠ final_state["final_report"] å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­
    long_term_memory = []
    long_term_memory.append({
        "topic": final_state["topic"],
        "report": final_state["final_report"]
    })
    print("\n\nğŸ§  [æ¼”ç¤º] æŠ¥å‘Šå·²å­˜å…¥'é•¿æœŸè®°å¿†'ã€‚")